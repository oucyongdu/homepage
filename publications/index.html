<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Yong Du&#39;s Homepage</title>
    <meta name="description" content="Yong Du&#39;s Homepage">
    <link rel="icon" href="/headlogo.png">
    
    <link rel="preload" href="/assets/css/0.styles.cedbd1ec.css" as="style"><link rel="preload" href="/assets/js/app.f18e34b1.js" as="script"><link rel="preload" href="/assets/js/3.1d9540bd.js" as="script"><link rel="preload" href="/assets/js/17.70a76ab9.js" as="script"><link rel="preload" href="/assets/js/11.86f09cba.js" as="script"><link rel="prefetch" href="/assets/js/1.6d469650.js"><link rel="prefetch" href="/assets/js/10.6b6bda37.js"><link rel="prefetch" href="/assets/js/12.64570833.js"><link rel="prefetch" href="/assets/js/13.7d4a7d2d.js"><link rel="prefetch" href="/assets/js/14.55beadba.js"><link rel="prefetch" href="/assets/js/15.6b4dfc0d.js"><link rel="prefetch" href="/assets/js/16.f073299e.js"><link rel="prefetch" href="/assets/js/18.ccd2278f.js"><link rel="prefetch" href="/assets/js/19.0a31b4fe.js"><link rel="prefetch" href="/assets/js/20.4704cbda.js"><link rel="prefetch" href="/assets/js/4.dd95573b.js"><link rel="prefetch" href="/assets/js/5.83a0ee55.js"><link rel="prefetch" href="/assets/js/6.0b2b6e2a.js"><link rel="prefetch" href="/assets/js/7.844f1cfe.js"><link rel="prefetch" href="/assets/js/8.dfb8a44a.js"><link rel="prefetch" href="/assets/js/9.c8f8c097.js">
    <link rel="stylesheet" href="/assets/css/0.styles.cedbd1ec.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar projects-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/logo.png" alt="Yong Du's Homepage" class="logo"> <span class="site-name can-hide">Yong Du's Homepage</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/publications/" class="nav-link router-link-exact-active router-link-active">Publications</a></div><div class="nav-item"><a href="/group/" class="nav-link">Group</a></div><div class="nav-item"><a href="/teaching/" class="nav-link">Teaching</a></div><div class="nav-item"><a href="/contact/" class="nav-link">Contact</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/publications/" class="nav-link router-link-exact-active router-link-active">Publications</a></div><div class="nav-item"><a href="/group/" class="nav-link">Group</a></div><div class="nav-item"><a href="/teaching/" class="nav-link">Teaching</a></div><div class="nav-item"><a href="/contact/" class="nav-link">Contact</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><p style="position:relative;"><font size="4"><sup>#</sup>Joint first author, *Corresponding author.</font></p> <h2 id="_2025">2025</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/TPAMI2025.jpg" alt></div> <div class="card-content"><p><strong>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</strong></p> <p><em><strong>Yong Du<sup>#</sup></strong>, Jiahui Zhan<sup>#</sup>, Xinzhe Li, Junyu Dong, Sheng Chen, Ming-Hsuan Yang, and Shengfeng He</em></p> <p>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2025</p> <p>[<a href="https://arxiv.org/pdf/2310.14222" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://zhanjiahui.github.io/UniTranslator/" target="_blank" rel="noopener noreferrer">Project Page<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/AAAI2025.jpg" alt></div> <div class="card-content"><p><strong>PersonaMagic: Stage-Regulated High-Fidelity Face Customization with Tandem Equilibrium</strong></p> <p><em>Xinzhe Li, Jiahui Zhan, Shengfeng He, Yangyang Xu, Junyu Dong, Huaidong Zhang, and <strong>Yong Du*</strong></em></p> <p>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025</p> <p>[<a href="https://arxiv.org/pdf/2412.15674" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/xzhe-Vision/PersonaMagic" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/TVCG2025.jpg" alt></div> <div class="card-content"><p><strong>StyleGAN-âˆž: Extending StyleGAN to Arbitrary-Ratio Translation with StyleBook</strong></p> <p><em>Yihua Dai, Tianyi Xiang, Bailin Deng, <strong>Yong Du</strong>, Hongmin Cai, Jing Qin, and Shengfeng He</em></p> <p>IEEE Transactions on Visualization and Computer Graphics (<strong>TVCG</strong>), 2025</p> <p>[PDF] [Code]</p></div></div> <h2 id="_2024">2024</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/ECCV2024.jpg" alt></div> <div class="card-content"><p><strong>D<sup>4</sup>-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On</strong></p> <p><em>Zhaotong Yang, Zicheng Jiang, Xinzhe Li, Huiyu Zhou, Junyu Dong, Huaidong Zhang, and <strong>Yong Du*</strong></em></p> <p>European Conference on Computer Vision (<strong>ECCV</strong>), 2024</p> <p>[<a href="https://arxiv.org/pdf/2407.15111" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/Jerome-Young/D4-VTON" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/CVPR2024.jpg" alt></div> <div class="card-content"><p><strong>D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval</strong></p> <p><em>Yi Xie, Yihong Lin, Wenjie Cai, Xuemiao Xu, Huaidong Zhang, <strong>Yong Du</strong>, and Shengfeng He</em></p> <p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</p> <p>[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_D3still_Decoupled_Differential_Distillation_for_Asymmetric_Image_Retrieval_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [Code]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Enhancing Generalized Zero-shot Learning with Dynamic Selective Knowledge Distillation</strong></p> <p><em>Weihua Lv, Yulong Zheng, Chao Liu, and <strong>Yong Du*</strong></em></p> <p>International Conference on Wireless Artificial Intelligent Computing Systems and Applications (WASA), 2024</p> <p>[PDF]</p></div></div> <h2 id="_2023">2023</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/CVPR2023.jpg" alt></div> <div class="card-content"><p><strong>Curricular Contrastive Regularization for Physics-aware Single Image Dehazing</strong></p> <p><em>Yu Zheng, Jiahui Zhan, Shengfeng He, Junyu Dong, and <strong>Yong Du*</strong></em></p> <p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023</p> <p>[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Curricular_Contrastive_Regularization_for_Physics-Aware_Single_Image_Dehazing_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Zheng_Curricular_Contrastive_Regularization_CVPR_2023_supplemental.pdf" target="_blank" rel="noopener noreferrer">Supp<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/YuZheng9/C2PNet" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/CVIU2023.jpg" alt></div> <div class="card-content"><p><strong>DSDNet: Toward Single Image Deraining with Self-paced Curricular Dual Stimulations</strong></p> <p><em><strong>Yong Du</strong>, Junjie Deng, Yulong Zheng, Junyu Dong, and Shengfeng He</em></p> <p>Computer Vision and Image Understanding (<strong>CVIU</strong>), 2023</p> <p>[<a href="https://www.sciencedirect.com/science/article/pii/S1077314223000371?dgcid=author" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="_2022">2022</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/ECCV2022.jpg" alt></div> <div class="card-content"><p><strong>Editing Out-of-domain GAN Inversion via Differential Activations</strong></p> <p><em>Haorui Song<sup>#</sup>, <strong>Yong Du<sup>#</sup></strong>, Tianyi Xiang, Junyu Dong, Jing Qin, and Shengfeng He</em></p> <p>European Conference on Computer Vision (<strong>ECCV</strong>), 2022</p> <p>[<a href="https://arxiv.org/abs/2207.08134" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://www.youtube.com/watch?v=aEM6mah60lc" target="_blank" rel="noopener noreferrer">Video Demo<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/HaoruiSong622/Editing-Out-of-Domain" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/TIP2022.jpg" alt></div> <div class="card-content"><p><strong>Pro-PULSE: Learning Progressive Encoders of Latent Semantics in GANs for Photo Upsampling</strong></p> <p><em>Yang Zhou, Yangyang Xu, <strong>Yong Du</strong>, Qiang Wen,  and Shengfeng He</em></p> <p>IEEE Transactions on Image Processing (<strong>TIP</strong>), 2022</p> <p>[<a href="https://ieeexplore.ieee.org/document/9678071" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/youngAt19/Pro-PULSE" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Delving Deep into Pixelized Face Recovery and Defense</strong></p> <p><em>Zhixuan Zhong, <strong>Yong Du</strong>, Yang Zhou, Jiangzhong Cao, and Shengfeng He</em></p> <p>Neurocomputing, 2022</p> <p>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222012395" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="_2021">2021</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/ICCV2021.jpg" alt></div> <div class="card-content"><p><strong>From Continuity to Editability: Inverting GANs with Consecutive Images</strong></p> <p><em>Yangyang Xu, <strong>Yong Du</strong>, Wenpeng Xiao, Xuemiao Xu, and Shengfeng He</em></p> <p>IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021</p> <p>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_From_Continuity_to_Editability_Inverting_GANs_With_Consecutive_Images_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Xu_From_Continuity_to_ICCV_2021_supplemental.pdf" target="_blank" rel="noopener noreferrer">Supp<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/CVPR2021.jpg" alt></div> <div class="card-content"><p><strong>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading</strong></p> <p><em>Sucheng Ren<sup>#</sup>, <strong>Yong Du<sup>#</sup></strong>, Jianming Lv, Guoqiang Han,  and Shengfeng He</em></p> <p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021</p> <p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/TOMM2021.jpg" alt></div> <div class="card-content"><p><strong>Invertible Grayscale with Sparsity Enforcing Priors</strong></p> <p><em><strong>Yong Du</strong>, Yangyang Xu, Taizhong Ye, Qiang Wen, Chufeng Xiao, Junyu Dong, Guoqiang Han, and Shengfeng He</em></p> <p>ACM Transactions on Multimedia Computing, Communications, and Applications (<strong>TOMM</strong>), 2021</p> <p>[<a href="https://dl.acm.org/doi/10.1145/3451993" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Mask-ShadowNet: Toward Shadow Removal via Masked Adaptive Instance Normalization</strong></p> <p><em>Shengfeng He, Bing Peng, Junyu Dong, and <strong>Yong Du*</strong></em></p> <p>IEEE Signal Processing Letters (SPL), 2021</p> <p>[<a href="https://ieeexplore.ieee.org/document/9408351" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Fast Scene Labeling via Structural Inference</strong></p> <p><em>Huaidong Zhang, Chu Han, Xiaodan Zhang, <strong>Yong Du</strong>, Xuemiao Xu, Guoqiang Han, Jing Qin, and Shengfeng He</em></p> <p>Neurocomputing, 2021</p> <p>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221003428" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>TPRDTVN: A Routing Algorithm in Delay Tolerant Vessel Network based on Long-term Trajectory Prediction</strong></p> <p><em>Chao Liu, Yingbin Li, Ruobing Jiang, <strong>Yong Du*</strong>, Qian Lu, and Zhongwen Guo</em></p> <p>Wireless Communications and Mobile Computing (WCMC), 2021</p> <p>[<a href="https://www.hindawi.com/journals/wcmc/2021/6630265/" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="_2020">2020</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/TMM2020.jpg" alt></div> <div class="card-content"><p><strong>Blind Image Denoising via Dynamic Dual Learning</strong></p> <p><em><strong>Yong Du</strong>, Guoqiang Han, Yinjie Tan, Chufeng Xiao, and Shengfeng He</em></p> <p>IEEE Transactions on Multimedia (<strong>TMM</strong>), 2020</p> <p>[<a href="https://ieeexplore.ieee.org/document/9136787" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/TIP2020.jpg" alt></div> <div class="card-content"><p><strong>Real-time Hierarchical Supervoxel Segmentation via a Minimum Spanning Tree</strong></p> <p><em>Bo Wang, Yiliang Chen, Wenxi Liu, Jing Qin, <strong>Yong Du</strong>, Guoqiang Han, and Shengfeng He</em></p> <p>IEEE Transactions on Image Processing (<strong>TIP</strong>), 2020</p> <p>[<a href="https://ieeexplore.ieee.org/document/9229239" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Invertible Grayscale via Dual Features Ensemble</strong></p> <p><em>Taizhong Ye<sup>#</sup>, <strong>Yong Du<sup>#</sup></strong>, Junjie Deng, and Shengfeng He</em></p> <p>IEEE Access, 2020</p> <p>[<a href="https://ieeexplore.ieee.org/document/9091800" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="before-2019">Before 2019</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/TCYB2019.jpg" alt></div> <div class="card-content"><p><strong>Exploiting Global Low-rank Structure and Local Sparsity Nature for Tensor Completion</strong></p> <p><em><strong>Yong Du</strong>, Guoqiang Han, Yuhui Quan, Zhiwen Yu, Hau-San Wong, C.L.Philip Chen, and Jun Zhang</em></p> <p>IEEE Transactions on Cybernetics (<strong>TCYB</strong>), 2019</p> <p>[<a href="https://ieeexplore.ieee.org/document/8418828" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/csyongdu/Exploiting-Global-Low-Rank-Structure-and-Local-Sparsity-Nature-for-Tensor-Completion" target="_blank" rel="noopener noreferrer">code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.f18e34b1.js" defer></script><script src="/assets/js/3.1d9540bd.js" defer></script><script src="/assets/js/17.70a76ab9.js" defer></script><script src="/assets/js/11.86f09cba.js" defer></script>
  </body>
</html>
